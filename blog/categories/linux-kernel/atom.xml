<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Linux-kernel | ykrocku]]></title>
  <link href="http://ykrocku.github.io/blog/categories/linux-kernel/atom.xml" rel="self"/>
  <link href="http://ykrocku.github.io/"/>
  <updated>2014-04-11T14:15:40+08:00</updated>
  <id>http://ykrocku.github.io/</id>
  <author>
    <name><![CDATA[ykrocku]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[深入分析diskstats]]></title>
    <link href="http://ykrocku.github.io/blog/2014/04/11/diskstats.markdown/"/>
    <updated>2014-04-11T09:21:09+08:00</updated>
    <id>http://ykrocku.github.io/blog/2014/04/11/diskstats.markdown</id>
    <content type="html"><![CDATA[<h2>背景</h2>

<p>内核很多重要子系统均通过proc文件的方式，将自身的一些统计信息输出，方便最终用户查看各子系统的运行状态，这些统计信息被称为metrics。
直接查看metrics并不能获取到有用的信息，一般都是由特定的应用程序(htop/sar/iostat等)每隔一段时间读取相关metrics，并进行相应计算，给出更具用户可读性的输出。
常见的metrics文件有：
* cpu调度统计信息的/proc/stat
* cpu负载统计信息的/proc/loadavg</p>

<p>通用块设备层也有一个重要的统计信息</p>

<ul>
<li>/proc/diskstats
内核通过diskstats文件，将通用块设备层的一些重要指标以文件的形式呈现给用户。</li>
</ul>


<p>因为本文档牵涉到通用块设备层很多细节，建议先了解IO调度器的相关知识。</p>

<h2>初探diskstats</h2>

<p>首先来看下diskstats里面都有些什么，下面截取的是一个diskstats文件内容：
```</p>

<h1>cat /proc/diskstats</h1>

<p>   8       0 sda 8567 1560 140762 3460 0 0 0 0 0 2090 3440
   8       1 sda1 8565 1557 140722 3210 0 0 0 0 0 1840 3190
   8      16 sdb 8157 1970 140762 2940 0 0 0 0 0 1710 2890
   8      17 sdb1 8155 1967 140722 2900 0 0 0 0 0 1670 2850
   8      32 sdc 8920 1574 206410 7870 430 0 461 250 0 6820 8120
   8      33 sdc1 8918 1571 206370 7840 430 0 461 250 0 6790 8090
   8      48 sdd 209703 1628 341966 1318450 3109063 331428 943042901 9728000 0 8943570 11015280
   8      49 sdd1 209701 1625 341926 1318200 3109063 331428 943042901 9728000 0 8943320 11015030
```
虽然如上面提到的，这些数字看上去完全没有规律。不过若想研究内核通用块设备层的统计实现方式，还是得一个一个字段的分析。</p>

<p>简单的说，每一行对应一个块设备，分别有ram0-ram15、loop0-loop7、mtdblock0-mtdblock5，剩下的sdxx就是硬盘和分区了。
这里以sda设备的数据为例，分别列举各字段的意义：
<code>
8       0 sda 8567 1560 140762 3460 0 0 0 0 0 2090 3440
</code></p>

<p>根据内核文档<a href="https://www.kernel.org/doc/Documentation/iostats.txt">iostats.txt</a>中描述，各字段意义如下：</p>

<table>
<thead>
<tr>
<th> 域    </th>
<th> Value  </th>
<th>  Quoted                               </th>
<th>  解释  </th>
</tr>
</thead>
<tbody>
<tr>
<td> F1    </td>
<td> 8      </td>
<td>  major number                         </td>
<td> 此块设备的主设备号 </td>
</tr>
<tr>
<td> F2    </td>
<td> 0      </td>
<td>  minor mumber                         </td>
<td> 此块设备的次设备号 </td>
</tr>
<tr>
<td> F3    </td>
<td> sda    </td>
<td>  device name                          </td>
<td> 此块设备名字 </td>
</tr>
<tr>
<td> F4    </td>
<td> 8567   </td>
<td>  reads completed successfully         </td>
<td> 成功完成的读请求次数 </td>
</tr>
<tr>
<td> F5    </td>
<td> 1560   </td>
<td>  reads merged                         </td>
<td> 读请求的次数 </td>
</tr>
<tr>
<td> F6    </td>
<td> 140762 </td>
<td>  sectors read                         </td>
<td> 读请求的扇区数总和 </td>
</tr>
<tr>
<td> F7    </td>
<td> 3460   </td>
<td>  time spent reading (ms)              </td>
<td> 读请求花费的时间总和 </td>
</tr>
<tr>
<td> F8    </td>
<td> 0      </td>
<td>  writes completed                     </td>
<td> 成功完成的写请求次数 </td>
</tr>
<tr>
<td> F9    </td>
<td> 0      </td>
<td>  writes merged                        </td>
<td> 写请求合并的次数 </td>
</tr>
<tr>
<td>F10    </td>
<td> 0      </td>
<td>  sectors written                      </td>
<td> 写请求的扇区数总和 </td>
</tr>
<tr>
<td>F11    </td>
<td> 0      </td>
<td>  time spent writing (ms)              </td>
<td> 写请求花费的时间总和 </td>
</tr>
<tr>
<td>F12    </td>
<td> 0      </td>
<td>  I/Os currently in progress           </td>
<td> 次块设备队列中的IO请求数 </td>
</tr>
<tr>
<td>F13    </td>
<td> 2090   </td>
<td>  time spent doing I/Os (ms)           </td>
<td> 块设备队列非空时间总和 </td>
</tr>
<tr>
<td>F14    </td>
<td> 3440   </td>
<td>  weighted time spent doing I/Os (ms)  </td>
<td> 块设备队列非空时间加权总和 </td>
</tr>
</tbody>
</table>


<p>基本上都是数量、时间的累加值，按照读、写分开统计。</p>

<h2>流程图</h2>

<p>下图是Linux内核通用块设备层IO请求处理的完整流程，如图例所示，所有的统计相关处理均有用不同颜色标注。
在进行深入分析前，请大致浏览图片，对整个流程有一个大致印象。
<img src="/images/LinuxBlockIO.png" title="[title text [alt text]]" ></p>

<h2>实现分析</h2>

<h3>proc入口</h3>

<p>在内核代码中grep &ldquo;diskstats"即可找到定义在block/genhd.c中的<code>diskstats_show</code>函数。
```c</p>

<pre><code>while ((hd = disk_part_iter_next(&amp;piter))) {
    cpu = part_stat_lock();
    part_round_stats(cpu, hd);
    part_stat_unlock();
    seq_printf(seqf, "%4d %7d %s %lu %lu %llu "
           "%u %lu %lu %llu %u %u %u %u\n",
           MAJOR(part_devt(hd)), MINOR(part_devt(hd)),
           disk_name(gp, hd-&gt;partno, buf),
           part_stat_read(hd, ios[READ]),
           part_stat_read(hd, merges[READ]),
           (unsigned long long)part_stat_read(hd, sectors[READ]),
           jiffies_to_msecs(part_stat_read(hd, ticks[READ])),
           part_stat_read(hd, ios[WRITE]),
           part_stat_read(hd, merges[WRITE]),
           (unsigned long long)part_stat_read(hd, sectors[WRITE]),
           jiffies_to_msecs(part_stat_read(hd, ticks[WRITE])),
           part_in_flight(hd),
           jiffies_to_msecs(part_stat_read(hd, io_ticks)),
           jiffies_to_msecs(part_stat_read(hd, time_in_queue))
        );
</code></pre>

<p>```
此段代码用seq_printf函数将保存在hd_struct结构体内的统计信息组成了diskstats文件。</p>

<h3>数据结构</h3>

<p>用到的数据结构都定义在&lt;linux/genhd.h>中，主要有disk_stats和hd_struct两个结构体，意义见注释：
```c
struct disk_stats {</p>

<pre><code>/*
 *sectors[0] &lt;--&gt; F6 
 *sectors[1] &lt;--&gt; F10
 */
unsigned long sectors[2];    /* READs and WRITEs */

/*
 *ios[0] &lt;--&gt; F4 
 *ios[1] &lt;--&gt; F8
 */
unsigned long ios[2];

/*
 *merges[0] &lt;--&gt; F5 
 *merges[1] &lt;--&gt; F9
 */
unsigned long merges[2];

/*
 *ticks[0] &lt;--&gt; F7 
 *ticks[1] &lt;--&gt; F11
 */
unsigned long ticks[2];

/*F13, time spent doing IOs*/
unsigned long io_ticks;

/*F14, weighted time spent doing I/Os (ms)  */
unsigned long time_in_queue;
</code></pre>

<p>};</p>

<p>struct hd_struct {</p>

<pre><code>unsigned long stamp;

/*F12 I/Os currently in progress*/
atomic_t in_flight[2];

/*如果支持SMP则需要使用“每CPU”变量，否则需要加锁*/
</code></pre>

<h1>ifdef  CONFIG_SMP</h1>

<pre><code>struct disk_stats __percpu *dkstats;
</code></pre>

<h1>else</h1>

<pre><code>struct disk_stats dkstats;
</code></pre>

<h1>endif</h1>

<pre><code>atomic_t ref;
struct rcu_head rcu_head;
</code></pre>

<p>};
```</p>

<h3>F7/F11 ticks</h3>

<p>见下一节</p>

<h3>F4/F8 ios</h3>

<p>如流程图所示，在每个IO结束后，都会调用blk_account_io_done函数，来对完成的IO进行统计。
blk_account_io_done统计了 <code>ios(F4/F8)</code>和<code>ticks(F7/F11)</code>，还处理了<code>in_flight</code>（后续节有分析）。
```c
static void blk_account_io_done(struct request *req)
{</p>

<pre><code>/*
 * 不统计Flush请求，见 http://en.wikipedia.org/wiki/Disk_buffer#Cache_flushing
 */
if (blk_do_io_stat(req) &amp;&amp; !(req-&gt;cmd_flags &amp; REQ_FLUSH_SEQ)) {
    /*
     * duration是当前时间（IO完成时间）减去此IO的入队时间（见流程图）
     */
    unsigned long duration = jiffies - req-&gt;start_time;

    /*从req获取请求类型：R / W*/
    const int rw = rq_data_dir(req);
    struct hd_struct *part;
    int cpu;

    cpu = part_stat_lock();
    /*获取请求对应的partition(part)*/
    part = req-&gt;part;
    /*
     * 该partition的ios[rw]加1
     * part_stat_inc定义在&lt;linux/genhd.h&gt;中
     * part_stat_inc这个宏用来处理SMP和非SMP的细节，见上面的结构体定义
     */
    part_stat_inc(cpu, part, ios[rw]);

    /*
     * 将此IO的存活时间加进ticks
     */
    part_stat_add(cpu, part, ticks[rw], duration);

    part_round_stats(cpu, part);

    /*
     *完成了一个IO，也就是in_flight（正在进行）的IO少了一个
     */
    part_dec_in_flight(part, rw);

    hd_struct_put(part);
    part_stat_unlock();
}
</code></pre>

<p>}
```</p>

<h3>F5/F9 merges</h3>

<p>内核每执行一次Back Merge或Front Merge，都会调用drive_stat_acct。
其实in_flight也是在这个函数中统计的，<code>new_io</code>参数用来区分是新的IO，如果不是新IO则是在merge的时候调用的：
```c
static void drive_stat_acct(struct request *rq, int new_io)
{</p>

<pre><code>struct hd_struct *part;
/*从req获取请求类型：R / W*/
int rw = rq_data_dir(rq);
int cpu;

if (!blk_do_io_stat(rq))
    return;

cpu = part_stat_lock();

if (!new_io) {
    part = rq-&gt;part;
    /*
     * 非新IO，merges[rw]加1
     */
    part_stat_inc(cpu, part, merges[rw]);
} else {
    .....
    part_round_stats(cpu, part);
    /*
     * 新提交了一个IO，也就是in_flight（正在进行）的IO多了一个
     */
    part_inc_in_flight(part, rw);
}

part_stat_unlock();
</code></pre>

<p>}
```</p>

<h3>F6/F10 sectors</h3>

<p>读写扇区总数是在blk_account_io_completion函数中统计的，如流程图中所示，这个函数在每次IO结束后调用：
```c
static void blk_account_io_completion(struct request *req, unsigned int bytes)
{</p>

<pre><code>if (blk_do_io_stat(req)) {
    const int rw = rq_data_dir(req);
    struct hd_struct *part;
    int cpu;

    cpu = part_stat_lock();
    part = req-&gt;part;
    /*
     *bytes是此IO请求的数据长度，右移9位等同于除以512，即转换成扇区数
     *然后加到sectors[rw]中
     */
    part_stat_add(cpu, part, sectors[rw], bytes &gt;&gt; 9);
    part_stat_unlock();
}
</code></pre>

<p>}
```</p>

<h3>F12 in_flight</h3>

<p>in_flight这个统计比较特别，因为其他统计都是计算累加值，而它是记录当前队列中IO请求的个数。统计方法则是：</p>

<ul>
<li>新IO请求插入队列（被merge的不算）后加1</li>
<li>完成一个IO后减1
实现见上面章节中的blk_account_io_done和drive_stat_acct函数内的注释。</li>
</ul>


<h3>F14 time_in_queue</h3>

<p>见下一节。</p>

<h3>F13 io_ticks</h3>

<p>io_ticks统计块设备请求队列非空的总时间，统计时间点与in_flight相同，统计代码实现在part_round_stats_single函数中：
```c
static void part_round_stats_single(int cpu, struct hd_struct *part,</p>

<pre><code>                unsigned long now)
</code></pre>

<p>{</p>

<pre><code>if (now == part-&gt;stamp)
    return;
/*
 *块设备队列非空
 */
if (part_in_flight(part)) {
    /*
     *统计加权时间 队列中IO请求个数 * 耗费时间
     */
    __part_stat_add(cpu, part, time_in_queue,
            part_in_flight(part) * (now - part-&gt;stamp));

    /*
     *统计队列非空时间
     */
    __part_stat_add(cpu, part, io_ticks, (now - part-&gt;stamp));
}
part-&gt;stamp = now;
</code></pre>

<p>}
```</p>

<p>整个代码实现的逻辑比较简单，在新IO请求插入队列（被merge不算），或完成一个IO请求的时候均执行如下操作：</p>

<ul>
<li><p>队列为空</p>

<ol>
<li>记下时间stamp = t1</li>
</ol>
</li>
<li><p>队列不为空</p>

<ol>
<li>io_ticks[rw]  += t2-t1</li>
<li>time_in_queue += in_flight * (t2-t1)</li>
<li>记下时间stamp = t2</li>
</ol>
</li>
</ul>


<p>下面是一个实际的例子，示例io_ticks和time_in_queue的计算过程：</p>

<table>
<thead>
<tr>
<th>  ID     </th>
<th>  Time       </th>
<th>   Ops             </th>
<th>  in_flight    </th>
<th>    stamp     </th>
<th>   gap     </th>
<th>  io_ticks   </th>
<th> time_in_queue    </th>
</tr>
</thead>
<tbody>
<tr>
<td>   0     </td>
<td>   100.00    </td>
<td> 新IO请求入队      </td>
<td>   0           </td>
<td>     0        </td>
<td>    &mdash;&ndash;    </td>
<td>   0         </td>
<td>  0               </td>
</tr>
<tr>
<td>   1     </td>
<td>   100.10    </td>
<td> 新IO请求入队      </td>
<td>   1           </td>
<td>     100.00   </td>
<td>    0.10   </td>
<td>   0.10      </td>
<td>  0.10            </td>
</tr>
<tr>
<td>   3     </td>
<td>   101.20    </td>
<td> 完成一个IO请求    </td>
<td>   2           </td>
<td>     100.10   </td>
<td>    0.80   </td>
<td>   1.20      </td>
<td>  1.70            </td>
</tr>
<tr>
<td>   4     </td>
<td>   103.50    </td>
<td> 完成一个IO请求    </td>
<td>   1           </td>
<td>     100.20   </td>
<td>    1.30   </td>
<td>   2.50      </td>
<td>  3.00            </td>
</tr>
<tr>
<td>   5     </td>
<td>   153.50    </td>
<td> 新IO请求入队      </td>
<td>   0           </td>
<td>     103.50   </td>
<td>    &mdash;&ndash;    </td>
<td>   2.50      </td>
<td>  3.00            </td>
</tr>
<tr>
<td>   6     </td>
<td>   154.50    </td>
<td> 完成一个IO请求    </td>
<td>   1           </td>
<td>     153.50   </td>
<td>    1.00   </td>
<td>   3.50      </td>
<td>  4.00            </td>
</tr>
</tbody>
</table>


<p>总共时间 54.50s， IO队列非空时间3.50s</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深入分析diskstats]]></title>
    <link href="http://ykrocku.github.io/blog/2014/04/11/diskstats/"/>
    <updated>2014-04-11T09:21:09+08:00</updated>
    <id>http://ykrocku.github.io/blog/2014/04/11/diskstats</id>
    <content type="html"><![CDATA[<h2>背景</h2>

<p>内核很多重要子系统均通过proc文件的方式，将自身的一些统计信息输出，方便最终用户查看各子系统的运行状态，这些统计信息被称为metrics。
直接查看metrics并不能获取到有用的信息，一般都是由特定的应用程序(htop/sar/iostat等)每隔一段时间读取相关metrics，并进行相应计算，给出更具用户可读性的输出。
常见的metrics文件有：</p>

<ul>
<li>cpu调度统计信息的/proc/stat</li>
<li>cpu负载统计信息的/proc/loadavg</li>
</ul>


<p>通用块设备层也有一个重要的统计信息</p>

<ul>
<li>/proc/diskstats
内核通过diskstats文件，将通用块设备层的一些重要指标以文件的形式呈现给用户。</li>
</ul>


<p>因为本文档牵涉到通用块设备层很多细节，建议先了解IO调度器的相关知识。</p>

<h2>初探diskstats</h2>

<p>首先来看下diskstats里面都有些什么，下面截取的是一个diskstats文件内容：
```</p>

<h1>cat /proc/diskstats</h1>

<p>   8       0 sda 8567 1560 140762 3460 0 0 0 0 0 2090 3440
   8       1 sda1 8565 1557 140722 3210 0 0 0 0 0 1840 3190
   8      16 sdb 8157 1970 140762 2940 0 0 0 0 0 1710 2890
   8      17 sdb1 8155 1967 140722 2900 0 0 0 0 0 1670 2850
   8      32 sdc 8920 1574 206410 7870 430 0 461 250 0 6820 8120
   8      33 sdc1 8918 1571 206370 7840 430 0 461 250 0 6790 8090
   8      48 sdd 209703 1628 341966 1318450 3109063 331428 943042901 9728000 0 8943570 11015280
   8      49 sdd1 209701 1625 341926 1318200 3109063 331428 943042901 9728000 0 8943320 11015030
```
虽然如上面提到的，这些数字看上去完全没有规律。不过若想研究内核通用块设备层的统计实现方式，还是得一个一个字段的分析。</p>

<p>简单的说，每一行对应一个块设备，分别有ram0-ram15、loop0-loop7、mtdblock0-mtdblock5，剩下的sdxx就是硬盘和分区了。
这里以sda设备的数据为例，分别列举各字段的意义：
<code>
8       0 sda 8567 1560 140762 3460 0 0 0 0 0 2090 3440
</code></p>

<p>根据内核文档<a href="https://www.kernel.org/doc/Documentation/iostats.txt">iostats.txt</a>中描述，各字段意义如下：</p>

<table>
<thead>
<tr>
<th> 域    </th>
<th> Value  </th>
<th>  Quoted                               </th>
<th>  解释  </th>
</tr>
</thead>
<tbody>
<tr>
<td> F1    </td>
<td> 8      </td>
<td>  major number                         </td>
<td> 此块设备的主设备号 </td>
</tr>
<tr>
<td> F2    </td>
<td> 0      </td>
<td>  minor mumber                         </td>
<td> 此块设备的次设备号 </td>
</tr>
<tr>
<td> F3    </td>
<td> sda    </td>
<td>  device name                          </td>
<td> 此块设备名字 </td>
</tr>
<tr>
<td> F4    </td>
<td> 8567   </td>
<td>  reads completed successfully         </td>
<td> 成功完成的读请求次数 </td>
</tr>
<tr>
<td> F5    </td>
<td> 1560   </td>
<td>  reads merged                         </td>
<td> 读请求的次数 </td>
</tr>
<tr>
<td> F6    </td>
<td> 140762 </td>
<td>  sectors read                         </td>
<td> 读请求的扇区数总和 </td>
</tr>
<tr>
<td> F7    </td>
<td> 3460   </td>
<td>  time spent reading (ms)              </td>
<td> 读请求花费的时间总和 </td>
</tr>
<tr>
<td> F8    </td>
<td> 0      </td>
<td>  writes completed                     </td>
<td> 成功完成的写请求次数 </td>
</tr>
<tr>
<td> F9    </td>
<td> 0      </td>
<td>  writes merged                        </td>
<td> 写请求合并的次数 </td>
</tr>
<tr>
<td>F10    </td>
<td> 0      </td>
<td>  sectors written                      </td>
<td> 写请求的扇区数总和 </td>
</tr>
<tr>
<td>F11    </td>
<td> 0      </td>
<td>  time spent writing (ms)              </td>
<td> 写请求花费的时间总和 </td>
</tr>
<tr>
<td>F12    </td>
<td> 0      </td>
<td>  I/Os currently in progress           </td>
<td> 次块设备队列中的IO请求数 </td>
</tr>
<tr>
<td>F13    </td>
<td> 2090   </td>
<td>  time spent doing I/Os (ms)           </td>
<td> 块设备队列非空时间总和 </td>
</tr>
<tr>
<td>F14    </td>
<td> 3440   </td>
<td>  weighted time spent doing I/Os (ms)  </td>
<td> 块设备队列非空时间加权总和 </td>
</tr>
</tbody>
</table>


<p>基本上都是数量、时间的累加值，按照读、写分开统计。</p>

<h2>流程图</h2>

<p>下图是Linux内核通用块设备层IO请求处理的完整流程，如图例所示，所有的统计相关处理均有用不同颜色标注。
在进行深入分析前，请大致浏览图片，对整个流程有一个大致印象。
<img src="/images/LinuxBlockIO.png" title="[title text [alt text]]" ></p>

<h2>实现分析</h2>

<h3>proc入口</h3>

<p>在内核代码中grep &ldquo;diskstats"即可找到定义在block/genhd.c中的<code>diskstats_show</code>函数。
```c</p>

<pre><code>while ((hd = disk_part_iter_next(&amp;piter))) {
    cpu = part_stat_lock();
    part_round_stats(cpu, hd);
    part_stat_unlock();
    seq_printf(seqf, "%4d %7d %s %lu %lu %llu "
           "%u %lu %lu %llu %u %u %u %u\n",
           MAJOR(part_devt(hd)), MINOR(part_devt(hd)),
           disk_name(gp, hd-&gt;partno, buf),
           part_stat_read(hd, ios[READ]),
           part_stat_read(hd, merges[READ]),
           (unsigned long long)part_stat_read(hd, sectors[READ]),
           jiffies_to_msecs(part_stat_read(hd, ticks[READ])),
           part_stat_read(hd, ios[WRITE]),
           part_stat_read(hd, merges[WRITE]),
           (unsigned long long)part_stat_read(hd, sectors[WRITE]),
           jiffies_to_msecs(part_stat_read(hd, ticks[WRITE])),
           part_in_flight(hd),
           jiffies_to_msecs(part_stat_read(hd, io_ticks)),
           jiffies_to_msecs(part_stat_read(hd, time_in_queue))
        );
</code></pre>

<p>```
此段代码用seq_printf函数将保存在hd_struct结构体内的统计信息组成了diskstats文件。</p>

<h3>数据结构</h3>

<p>用到的数据结构都定义在&lt;linux/genhd.h>中，主要有disk_stats和hd_struct两个结构体，意义见注释：
```c
struct disk_stats {</p>

<pre><code>/*
 *sectors[0] &lt;--&gt; F6 
 *sectors[1] &lt;--&gt; F10
 */
unsigned long sectors[2];    /* READs and WRITEs */

/*
 *ios[0] &lt;--&gt; F4 
 *ios[1] &lt;--&gt; F8
 */
unsigned long ios[2];

/*
 *merges[0] &lt;--&gt; F5 
 *merges[1] &lt;--&gt; F9
 */
unsigned long merges[2];

/*
 *ticks[0] &lt;--&gt; F7 
 *ticks[1] &lt;--&gt; F11
 */
unsigned long ticks[2];

/*F13, time spent doing IOs*/
unsigned long io_ticks;

/*F14, weighted time spent doing I/Os (ms)  */
unsigned long time_in_queue;
</code></pre>

<p>};</p>

<p>struct hd_struct {</p>

<pre><code>unsigned long stamp;

/*F12 I/Os currently in progress*/
atomic_t in_flight[2];

/*如果支持SMP则需要使用“每CPU”变量，否则需要加锁*/
</code></pre>

<h1>ifdef  CONFIG_SMP</h1>

<pre><code>struct disk_stats __percpu *dkstats;
</code></pre>

<h1>else</h1>

<pre><code>struct disk_stats dkstats;
</code></pre>

<h1>endif</h1>

<pre><code>atomic_t ref;
struct rcu_head rcu_head;
</code></pre>

<p>};
```</p>

<h3>F7/F11 ticks</h3>

<p>见下一节</p>

<h3>F4/F8 ios</h3>

<p>如流程图所示，在每个IO结束后，都会调用blk_account_io_done函数，来对完成的IO进行统计。
blk_account_io_done统计了 <code>ios(F4/F8)</code>和<code>ticks(F7/F11)</code>，还处理了<code>in_flight</code>（后续节有分析）。
```c
static void blk_account_io_done(struct request *req)
{</p>

<pre><code>/*
 * 不统计Flush请求，见 http://en.wikipedia.org/wiki/Disk_buffer#Cache_flushing
 */
if (blk_do_io_stat(req) &amp;&amp; !(req-&gt;cmd_flags &amp; REQ_FLUSH_SEQ)) {
    /*
     * duration是当前时间（IO完成时间）减去此IO的入队时间（见流程图）
     */
    unsigned long duration = jiffies - req-&gt;start_time;

    /*从req获取请求类型：R / W*/
    const int rw = rq_data_dir(req);
    struct hd_struct *part;
    int cpu;

    cpu = part_stat_lock();
    /*获取请求对应的partition(part)*/
    part = req-&gt;part;
    /*
     * 该partition的ios[rw]加1
     * part_stat_inc定义在&lt;linux/genhd.h&gt;中
     * part_stat_inc这个宏用来处理SMP和非SMP的细节，见上面的结构体定义
     */
    part_stat_inc(cpu, part, ios[rw]);

    /*
     * 将此IO的存活时间加进ticks
     */
    part_stat_add(cpu, part, ticks[rw], duration);

    part_round_stats(cpu, part);

    /*
     *完成了一个IO，也就是in_flight（正在进行）的IO少了一个
     */
    part_dec_in_flight(part, rw);

    hd_struct_put(part);
    part_stat_unlock();
}
</code></pre>

<p>}
```</p>

<h3>F5/F9 merges</h3>

<p>内核每执行一次Back Merge或Front Merge，都会调用drive_stat_acct。
其实in_flight也是在这个函数中统计的，<code>new_io</code>参数用来区分是新的IO，如果不是新IO则是在merge的时候调用的：
```c
static void drive_stat_acct(struct request *rq, int new_io)
{</p>

<pre><code>struct hd_struct *part;
/*从req获取请求类型：R / W*/
int rw = rq_data_dir(rq);
int cpu;

if (!blk_do_io_stat(rq))
    return;

cpu = part_stat_lock();

if (!new_io) {
    part = rq-&gt;part;
    /*
     * 非新IO，merges[rw]加1
     */
    part_stat_inc(cpu, part, merges[rw]);
} else {
    .....
    part_round_stats(cpu, part);
    /*
     * 新提交了一个IO，也就是in_flight（正在进行）的IO多了一个
     */
    part_inc_in_flight(part, rw);
}

part_stat_unlock();
</code></pre>

<p>}
```</p>

<h3>F6/F10 sectors</h3>

<p>读写扇区总数是在blk_account_io_completion函数中统计的，如流程图中所示，这个函数在每次IO结束后调用：
```c
static void blk_account_io_completion(struct request *req, unsigned int bytes)
{</p>

<pre><code>if (blk_do_io_stat(req)) {
    const int rw = rq_data_dir(req);
    struct hd_struct *part;
    int cpu;

    cpu = part_stat_lock();
    part = req-&gt;part;
    /*
     *bytes是此IO请求的数据长度，右移9位等同于除以512，即转换成扇区数
     *然后加到sectors[rw]中
     */
    part_stat_add(cpu, part, sectors[rw], bytes &gt;&gt; 9);
    part_stat_unlock();
}
</code></pre>

<p>}
```</p>

<h3>F12 in_flight</h3>

<p>in_flight这个统计比较特别，因为其他统计都是计算累加值，而它是记录当前队列中IO请求的个数。统计方法则是：</p>

<ul>
<li>新IO请求插入队列（被merge的不算）后加1</li>
<li>完成一个IO后减1
实现见上面章节中的blk_account_io_done和drive_stat_acct函数内的注释。</li>
</ul>


<h3>F14 time_in_queue</h3>

<p>见下一节。</p>

<h3>F13 io_ticks</h3>

<p>io_ticks统计块设备请求队列非空的总时间，统计时间点与in_flight相同，统计代码实现在part_round_stats_single函数中：
```c
static void part_round_stats_single(int cpu, struct hd_struct *part,</p>

<pre><code>                unsigned long now)
</code></pre>

<p>{</p>

<pre><code>if (now == part-&gt;stamp)
    return;
/*
 *块设备队列非空
 */
if (part_in_flight(part)) {
    /*
     *统计加权时间 队列中IO请求个数 * 耗费时间
     */
    __part_stat_add(cpu, part, time_in_queue,
            part_in_flight(part) * (now - part-&gt;stamp));

    /*
     *统计队列非空时间
     */
    __part_stat_add(cpu, part, io_ticks, (now - part-&gt;stamp));
}
part-&gt;stamp = now;
</code></pre>

<p>}
```</p>

<p>整个代码实现的逻辑比较简单，在新IO请求插入队列（被merge不算），或完成一个IO请求的时候均执行如下操作：</p>

<ul>
<li><p>队列为空</p>

<ol>
<li>记下时间stamp = t1</li>
</ol>
</li>
<li><p>队列不为空</p>

<ol>
<li>io_ticks[rw]  += t2-t1</li>
<li>time_in_queue += in_flight * (t2-t1)</li>
<li>记下时间stamp = t2</li>
</ol>
</li>
</ul>


<p>下面是一个实际的例子，示例io_ticks和time_in_queue的计算过程：</p>

<table>
<thead>
<tr>
<th>  ID     </th>
<th>  Time       </th>
<th>   Ops             </th>
<th>  in_flight    </th>
<th>    stamp     </th>
<th>   gap     </th>
<th>  io_ticks   </th>
<th> time_in_queue    </th>
</tr>
</thead>
<tbody>
<tr>
<td>   0     </td>
<td>   100.00    </td>
<td> 新IO请求入队      </td>
<td>   0           </td>
<td>     0        </td>
<td>    &mdash;&ndash;    </td>
<td>   0         </td>
<td>  0               </td>
</tr>
<tr>
<td>   1     </td>
<td>   100.10    </td>
<td> 新IO请求入队      </td>
<td>   1           </td>
<td>     100.00   </td>
<td>    0.10   </td>
<td>   0.10      </td>
<td>  0.10            </td>
</tr>
<tr>
<td>   3     </td>
<td>   101.20    </td>
<td> 完成一个IO请求    </td>
<td>   2           </td>
<td>     100.10   </td>
<td>    0.80   </td>
<td>   1.20      </td>
<td>  1.70            </td>
</tr>
<tr>
<td>   4     </td>
<td>   103.50    </td>
<td> 完成一个IO请求    </td>
<td>   1           </td>
<td>     100.20   </td>
<td>    1.30   </td>
<td>   2.50      </td>
<td>  3.00            </td>
</tr>
<tr>
<td>   5     </td>
<td>   153.50    </td>
<td> 新IO请求入队      </td>
<td>   0           </td>
<td>     103.50   </td>
<td>    &mdash;&ndash;    </td>
<td>   2.50      </td>
<td>  3.00            </td>
</tr>
<tr>
<td>   6     </td>
<td>   154.50    </td>
<td> 完成一个IO请求    </td>
<td>   1           </td>
<td>     153.50   </td>
<td>    1.00   </td>
<td>   3.50      </td>
<td>  4.00            </td>
</tr>
</tbody>
</table>


<p>总共时间 54.50s， IO队列非空时间3.50s</p>
]]></content>
  </entry>
  
</feed>
